#Loading all the necessary libraries 
library(tm)
library(ggplot2)
library(caret)
library(lattice)
library(quanteda)
library(tidyr)
library(topicmodels)
library(tidytext)
library(dplyr)
library(ldatuning)
library(stringr)
#Reading in the data in the form of a csv file
  model.raw<-read.csv("Target_comments.csv", stringsAsFactors = FALSE, fileEncoding = "latin1")

# Adding a new feature as a new coloumn
  model.raw$TextLength<-nchar(model.raw$Comments)

#Removing certain words not relevant for analysis

  model.raw$Comments<-gsub("Card", "", model.raw$Comments)
  model.raw$Comments<-gsub("Target", "", model.raw$Comments)
  model.raw$Comments<-gsub("card", "", model.raw$Comments)
  model.raw$Comments<-gsub("target", "", model.raw$Comments)

#Creating a corpus
  my_source<-VectorSource(model.raw$Comments)
  corpus<-Corpus(my_source)
  corpus<- tm_map(corpus, tolower)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  corpus <- tm_map(corpus, stripWhitespace)
  corpus<- tm_map(corpus, removeNumbers)
  corpus<- tm_map(corpus, stemDocument)

  dtm <- DocumentTermMatrix(corpus, control=list(minDocFreq=2, minWordLength=2))

#Remove empty cells and create a new corpus that aligns with the processed dtm
  rowTotals <- apply(dtm , 1, sum)
  empty.rows<-dtm[rowTotals == 0,]$dimnames[1][[1]]
  empty.rows<-as.numeric(empty.rows)
  corpus <- corpus[-empty.rows]
  corpus.df<-as.data.frame(corpus$content)
  dtm <- DocumentTermMatrix(corpus, control=list(minDocFreq=2, minWordLength=2))
  x<-length(as.numeric(empty.rows))
  delete.rows<-as.numeric(empty.rows)

  delete.rows[1]
  for (i in 1:x){
  model.raw<-model.raw[-delete.rows[i],]
  i<-i+1
  }

#Validation check to ensure corpus.df and model.raw match up - 743 can be any n
  corpus.df[743,]
  model.raw[743,]$Comments


#Merge processed datasets
  final<-bind_cols(model.raw,corpus.df)

#Finding out how many topics to create
  result<-FindTopicsNumber(dtm,
                           topics=seq(from = 2, to =15, by=1),
                           metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
                           method = "Gibbs",
                           control = list(seed = 77),
                           mc.cores = 3L,
                           verbose = TRUE)

  FindTopicsNumber_plot(result)

#Create topics using LDA algorithm
  lda<-LDA(dtm, k=7, control = list(seed=2343))



#Using tidytext manifest topics
  topics<-tidy(lda, matrix="beta")
  topics

#Showing the top terms and grouping them by topics created - Using dplyr's top_n limiting to 10
  top_terms<-topics %>%
    group_by(topic) %>% 
    top_n(10, beta) %>% 
    ungroup() %>% 
    arrange(topic, -beta)

#Visualization of the top terms.
  top_terms %>%
    mutate(term = reorder(term, beta)) %>%
    ggplot(aes(term, beta), fill = factor(topic)) +
    theme_bw()+
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    coord_flip()


#Creating a per-document-per-topic probability list
  documents<-tidy(lda, matrix="gamma")


#Extract numbers from the document column
  documents$doc.num<-as.numeric(str_extract(documents$document, "[0-9]+"))


#Finding the highest probablity gamma score to assign topic number
  i<-1
  max_pos<-vector()
  for(i in documents$document){

   x<-filter(documents, document==i)
   max<-max(x$gamma) 
   max_pos[length(max_pos)+1]<-max


  }

  documents$max<-max_pos
  documents$diff<-documents$gamma-documents$max
  documents$corpus.df<-corpus.df$Content
  documents<-filter(documents, diff==0)

#Export results 
  write.csv(documents, file="documents-K7.csv")
  write.csv(model.raw, file="model.raw.csv")


